
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Mask3D">
  <meta name="keywords" content="OpenSun3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ü§ñ AIE2025</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AIE 2025<br />
              <p class="title is-4 publication-title"> <p class="title is-3 publication-title">The First International Symposium on Artificial Intelligence for
Engineering Innovations
              </p>
              
            </h1>
            <h1 class="is-is-5" style="color: #5c5c5c;">Abu Dhabi, UAE.</h1>
            <b>Time - Monday, February 12-13th, 8:30-12:00 + 14:30-18:00</b>
            <br>
            <b>Location - TBD</b>
            <br>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#overview" class="button">Overview</a>
          <a href="#schedule" class="button">Schedule</a>
          <a href="#speakers" class="button">Speakers</a>
          <a href="#dates" class="button">Dates</a>
          <!-- <a href="#accepted" class="button">Accepted Papers</a> -->
          <!-- <a href="#challenge" class="button">Challenge</a> -->
          <a href="#organizers" class="button">Organizers</a>
          <div class="dropdown">
            <span class="button"><i class="fa fa-bars"></i></span>
            <div class="dropdown-content">
              <a href="https://2024.ieeeicip.org/">ICIP 2024</a>
              <!-- <a href="index_iccv23.html">ICCV 2023</a> -->
            </div>
        </h2>
        <!-- <h3 class="subtitle has-text-centered">

        </h3> -->
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title" id="overview">Overview üí°</h2>
          <div class="content has-text-justified">
            <p>Artificial Intelligence (AI) has proven to be a powerful tool in addressing complex challenges across a broad
spectrum of field. The AIE 2025 Symposium is set to convene researchers from diverse backgrounds, spanning
fundamental AI research and various engineering disciplines including but not limited to Cybersecurity, Urban
Science, Robotics, Water and Environment, Materials, Bioinnovation, Resilient Systems, Quantum Computing,
Wireless Communications, and Renewable Energy. This symposium is dedicated to discussing, exploring, and
advancing the use of Artificial Intelligence to tackle challenges within these pivotal engineering research areas.
Scheduled for February 12-13, 2025, the event seeks to foster interdisciplinary collaboration, drive innovation, and
facilitate the sharing of insights on AI's role in propelling engineering forward.
            </p>

          </div>
        </div>
      </section>

      <!-- <section class="section" id="News">
        <div class="container is-max-desktop content">
          <h2 class="title">News üì∞</h2>
          <div class="content has-text-justified">
            <ul>
              <li><b>April 2023</b>: Workshop website is online.</li>
            </ul>
          </div>
        </div>
      </section> -->
      <section class="section" style="margin-top: -50px">
        <div class="container is-max-desktop">
          <section class="section" id="Schedule">
            <div class="container is-max-desktop content">
              <h2 class="title" id="schedule">Schedule ‚è∞ (tentative)</h2>
              <div class="content has-text-justified">
                <table class="table table-striped" style="font-size: 15px;"> <!-- Reduced font size -->
                  <thead>
                    <tr>
                      <th width="120">Time</th>
                      <th width="550">Topic</th>
                      <th>Speaker</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>8:30 - 8:35</td>
                      <td style="background-color:#cae1ff">Opening Remark</td>
                      <td>Yi Fang</td>
                    </tr>
                    <tr>
                      <td>8:35 - 9:00</td>
                      <td style="background-color:#ffe0c6">AI for Engineering Innovation</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>9:00 - 9:30</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>9:30 - 10:00</td>
                      <td style="background-color:#ffe0c6">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>10:00 - 10:30</td>
                      <td style="background-color:#eeeeee">Coffee Break</td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>10:30 - 11:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>11:00 - 11:30</td>
                      <td style="background-color:#ffe0c6">TBDs</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>11:30 - 12:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>12:00 - 14:30</td>
                      <td style="background-color:#ffffff">Lunch</td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>14:30 - 15:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>15:00 - 15:30</td>
                      <td style="background-color:#ffe0c6">TBDI</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>15:30 - 16:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>16:00 - 16:30</td>
                      <td style="background-color:#eeeeee">Coffee Break</td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>16:30 - 17:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>17:00 - 17:30</td>
                      <td style="background-color:#ffe0c6">TBD</td>
                      <td>TBD</td>
                    </tr>
                    <tr>
                      <td>17:30 - 18:00</td>
                      <td style="background-color:#cae1ff">TBD</td>
                      <td>TBD</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </section>
        </div>
      </section>
      
      
      

          <section class="section" id="Invited Speakers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="speakers">Invited Speakers üßë‚Äçüè´</h2>


              <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html" target="_blank">
                <div class="card">
                  <div class="card-content">
                    <div class="columns is-vcentered">
                      <div class="column is-one-quarter">
                        <figure class="image is-128x128">
                          <img class="is-rounded" src="static/images/fang.jpg">
                        </figure>
                      </div>
                      <div class="column">
                        <p class="title is-4">Yi Fang</p>
                        <p class="subtitle is-6">Associate Professor, New York University</p>
                      </div>
                    </div>
                    <div class="content">
                    Dr. Yi Fang, is an Associate Professor of Electrical and Computer Engineering and an Affiliated Associate Professor of Computer Science at NYU and NYU Abu Dhabi, as well as a member of the Center for Artificial Intelligence and Robotics (CAIR) at NYU. After earning his doctorate from Purdue University with a focus on computer graphics and vision, he gained industry experience at Siemens and Riverain Technologies, and academic experience at Vanderbilt University. His research focuses on embodied AI, general-purpose robots, and humanoids, with applications spanning engineering, social science, medicine, and biology. Dr. Fang founded the NYU AIR Lab (Embodied AI and Robotics Lab), a leading center for research in robotics and AI.                     </div>
                  </div>
                </div>
              </a>    


             
              <br /><!--
          <a href="https://www.professoren.tum.de/en/dai-angela">
            <div class="card">
              <div class="card-content">

                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <figure class="image is-128x128">
                      <img class="is-rounded" src="./static/img/angela.jpg" alt="Placeholder image">
                    </figure>
                  </div>
                  <div class="column">
                    <p class="title is-4">Professor Angela Dai</p>
                    <p class="subtitle is-6">Technical University of Munich</p>
                  </div>
                </div>

                <div class="content">
                  Angela Dai is an assistant professor at the Technical University of Munich (TUM) where she leads the
                  3D AI Lab.
                  Her research focuses on understanding how the 3D world around us can be modeled and semantically
                  understood.
                  Prof. Dai is the creator of the seminal ScanNet benchmark that sparked the development of numerous 3D
                  scene understanding works.
                </div>
              </div>
            </div>
          </a>
          <br />
          <a href="https://www.sfu.ca/computing/people/faculty/manolissavva.html">
            <div class="card">
              <div class="card-content">

                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <figure class="image is-128x128">
                      <img class="is-rounded" src="./static/img/manolis.jpg" alt="Placeholder image">
                    </figure>
                  </div>
                  <div class="column">
                    <p class="title is-4">Professor Manolis Savva</p>
                    <p class="subtitle is-6">Simon Fraser University</p>
                  </div>
                </div>

                <div class="content">
                  Manolis Savva is an assistant professor in the School of Computing Science at Simon Fraser University,
                  and a Canada Research Chair in Computer Graphics.
                  His research focuses on analysis, organization and generation of 3D content.
                  The methods that he works on are stepping stones towards holistic 3D scene understanding revolving
                  around people,
                  with applications in computer graphics, computer vision, and robotics.
                  Prof. Savva contributed highly influential works towards embodied AI including
                  Matterport and Habitat.
                </div>
              </div>
            </div>
          </a>
          <br />
          <a href="https://www.cs.princeton.edu/~funk/">
            <div class="card">
              <div class="card-content">

                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <figure class="image is-128x128">
                      <img class="is-rounded" src="./static/img/thomas.jpg" alt="Placeholder image">
                    </figure>
                  </div>
                  <div class="column">
                    <p class="title is-4">Professor Thomas Funkhouser</p>
                    <p class="subtitle is-6">Google / Princeton University</p>
                  </div>
                </div>

                <div class="content">
                  Thomas Funkhouser is a full professor at Princeton University and a senior research scientist at
                  Google.
                  His research focuses on computer graphics, computer vision, and in particular 3D machine perception.
                  In recent years, Professor Funkhouser has greatly impacted the field of 3D scene understanding.
                </div>
              </div>
            </div>
          </a>
          <br />
          <a href="https://itee.uq.edu.au/profile/8084/jen-jen-chung">
            <div class="card">
              <div class="card-content">

                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <figure class="image is-128x128">
                      <img class="is-rounded" src="./static/img/jenjen.jpeg" alt="Placeholder image">
                    </figure>
                  </div>
                  <div class="column">
                    <p class="title is-4">Professor Jen Jen Chung</p>
                    <p class="subtitle is-6">University of Queensland</p>
                  </div>
                </div>

                <div class="content">
                  Jen Jen Chung is an associate professor in Mechatronics within the School of Information Technology
                  and Electrical Engineering at the University of Queensland.
                  Her current research interests include perception, planning and learning for robotic mobile
                  manipulation, algorithms for robot navigation through human crowds, informative path planning and
                  adaptive sampling.
                </div>
              </div>
            </div>
          </a>
            </div>
          </section>

          <section class="section" id="Invited Speakers">
            <div class="container is-max-desktop content">
              <!-- <h2 class="title" id="relatedwork">Related Works üßë‚Äçü§ù</h2>
              Below is a collection of concurrent and related works in the field of open-set 3D scene understanding.
              Please
              feel free to get in touch to add other works as well.
              <ul>
                <li><a href="https://concept-fusion.github.io/">ConceptFusion: Open-set Multimodal 3D Mapping</a> RSS'23
                </li>
                <li><a href="https://pengsongyou.github.io/openscene">OpenScene: 3D Scene Understanding with Open
                    Vocabularies</a> CVPR'23</li>
                <li><a href="https://www.lerf.io">LERF: Language Embedded Radiance Fields</a> ICCV'23</li>
                <li><a href="https://pfnet-research.github.io/distilled-feature-fields/">Decomposing NeRF for Editing
                    via Feature Field Distillation</a> NeurIPS'22</li>
                <li><a href="https://semantic-abstraction.cs.columbia.edu/">Semantic Abstraction: Open-World 3D Scene
                    Understanding from 2D Vision-Language Models</a> CoRL'22</li>
                <li><a href="https://rozdavid.github.io/scannet200">Language-Grounded Indoor 3D Semantic Segmentation in
                    the Wild</a> ECCV'22</li>
                <li><a href="https://3dlg-hcvc.github.io/multiscan/">MultiScan: Scalable RGBD Scanning for 3D
                    Environments with Articulated Objects</a> NeurIPS'22</li>
                <li><a href="https://github.com/NVlabs/ODISE">ODISE: Open-Vocabulary Panoptic Segmentation with
                    Text-to-Image Diffusion Models</a> CVPR'23</li>
                <li><a href="https://github.com/Kunhao-Liu/3D-OVS">Weakly Supervised 3D Open-Vocabulary Segmentation</a>
                  NeurIPS'23</li>
                <li><a href="https://arxiv.org/abs/2306.02329">Multi-CLIP: Contrastive Vision-Language Pre-training for
                    Question Answering tasks in 3D Scenes</a></li>
                <li><a href="https://openmask3d.github.io">OpenMask3D: Open-Vocabulary 3D Instance Segmentation</a>
                  NeurIPS'23</li>
                <li><a href="https://arxiv.org/abs/2303.04748">CLIP-FO3D: Learning Free Open-world 3D Scene
                    Representations from 2D Dense CLIP</li>
                <li><a href="https://tsagkas.github.io/vl-fields/">VL-Fields: Towards Language-Grounded Neural Implicit
                    Spatial Representations</a> ICRA'23</li>
                <li><a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">PLA:
                    Language-Driven Open-Vocabulary 3D Scene Understanding</a> CVPR'23</li>
                <li><a href="https://arxiv.org/abs/2304.00962">RegionPLC: Regional Point-Language Contrastive Learning
                    for Open-World 3D Scene Understanding</a></li>
                <li><a href="https://arxiv.org/abs/2309.00616">OpenIns3D: Snap and Lookup for 3D open-vocabulary
                    Instance Segmentation</a></li>
                <li><a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf">ConceptGraphs:
                    Open-Vocabulary 3D Scene Graphs for Perception and Planning</a></li>
                <li><a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Open-Vocabulary
                    Point-Cloud Object Detection without 3D Annotation</a> CVPR'23</li>
                <li><a href="https://github.com/yangcaoai/CoDA_NeurIPS2023">CoDA: Collaborative Novel
                    Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</a> NeurIPS'23
                </li>
                <br>
              </ul>
              and many more ... -->


              </ul>
            </div>
          </section>
          
          <section class="section" style="margin-top: -50px">
            <div class="container is-max-desktop">
              <section class="section" id="Motivation">
                <div class="container is-max-desktop content">
                  <h2 class="title" id="overview">Call for Papers üìù</h2>
                  <div class="content has-text-justified">
                  <p>Topics:</p>
                    <li>Embodied AI and Foundation Models for Next-Generation Autonomous Systems</li> 
<li>Ethics and Bias Mitigation in AI: Developing Fair and Transparent Algorithms</li> 
<li>Reinforcement Learning: Techniques for Adaptive AI Systems</li> 
<li>AI-driven cybersecurity solutions and their implications for protecting complex systems</li> 
<li>Urban science and the role of AI in developing smart city infrastructures</li> 
<li>Robotics innovations, focusing on AI applications for improved efficiency and adaptability</li> 
<li>Clinical Computing or Wearable Computing</li> 
<li>Water and environmental management using AI for sustainable resource allocation</li> 
<li>Materials science advancements through AI for discovering new materials</li> 
<li>Bioinnovation, leveraging AI for breakthroughs in medicine and genetics</li> 
<li>Resilient systems design, enhanced by AI for better disaster preparedness</li> 
<li>Quantum computing applications in AI, pushing computational boundaries</li> 
<li>Wireless communications optimization through AI for enhanced connectivity</li> 
<li>Renewable energy management, using AI for efficient energy production and distribution</li> 
<li>Machine learning and deep learning techniques for engineering problem-solving</li> 
<li>AI in the design and operation of autonomous vehicles and drones</li> 
<li>The intersection of AI and quantum computing for engineering applications</li> 
<li>Big data analytics and AI for predictive maintenance in engineering infrastructures</li> 
<li>Human-AI interaction and its impact on engineering design and innovation</li> 
<li>Ethics, privacy, and security in AI applications within engineering projects</li> 
<li>Collaborative AI and multi-agent systems for complex engineering tasks</li> 
<li>AI applications in civil and structural engineering for smarter construction</li> 
                    
                  <br>
                  <p>Selected papers will earn the opportunity for presentation in the form of either posters or spotlight talks during the workshop. Additionally, these papers will be published and made accessible via IEEE Xplore, adhering to the <a href ="https://cmsworkshops.com/ICIP2024/papers/paper_kit.php"> ICIP's guidelines </a> for workshop contributions. Please note, as per ICIP regulations, at least one author from each accepted paper is required to complete an in-person registration for the conference.</p>
                  <p>The submission deadline is April 25, 2024 (Anywhere on Earth). Papers should be no longer than 4 pages (excluding references) and styled in the ICIP <a href = "https://cmsworkshops.com/ICIP2024/papers/paper_kit.php">format</a>.</p>
                  <p>Submission Link : <a href = "https://urldefense.proofpoint.com/v2/url?u=https-3A__cmsworkshops.com_ICIP2024_papers_submission.asp-3FType-3DWS-26ID-3D4&d=DwIFaQ&c=slrrB7dE8n7gBJbeO0g-IQ&r=Nr6S6uI8GLu6TmJ7zqgFrA&m=iqxFpl5jtpkohRzLo3ZS-o6rPlDgYuz09yQjGisvyexypnagV8jAsC7RYz4Dra35&s=GMettIFex-NL1WZjDE2ItlJS2JG63DOkzBeyykhwiE0&e="><b>Link</b></a></p>
                  </div>
                </div>
              </section>         
          
          <section class="section" id="Papers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="dates">Important Dates üóìÔ∏è</h2>
              <ul>
                <!-- <li><b>Paper Submission Deadline</b> -->
                  <!-- : We accept novel full 8-page papers for publication in the proceedings, and
                  either shorter
                  4-page extended abstracts or 8-page papers of novel or previously published work that will not
                  be included in
                  the
                  proceedings. All submissions have to follow the <a
                    href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines">CVPR 2024 author
                    guidelines.</a>
                </li>
                <ul>
                  <li><b>Submission Portal</b>: <a href="https://cmt3.research.microsoft.com/OpenSUN3D2024">CMT</a></li> -->
                  <li><b>Paper Submission Deadline</b>: April 25, 2024.</li>
                  <li><b>Paper Acceptance Notification</b>: June 6, 2024</li>
                  <li><b>Final Paper Submission Deadlin</b>: June 19, 2024</li>
                  <li><b>Author Registration Deadline</b>: July 11, 2024</li>
                </ul>
                <!-- <li><b>Challenge Track 1</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_1">Open-vocabulary 3D object instance
                    search</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li>

                </ul>
                <li><b>Challenge Track 2</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_2">Open-vocabulary 3D affordance
                    grounding</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li> -->

                </ul>
              </ul>
            </div>
          </section>

          <!-- <section class="section" id="Challenge">
            <div class="container is-max-desktop content">
              <h2 class="title" id="challenge">Challenge</h2>
              (Please check <a href="https://opensun3d.github.io/index_iccv23.html#challengeresults">this page</a> out
              for an overview of last year's challenge results.
              We have also published a <a href="https://arxiv.org/abs/2402.15321"> technical report</a> providing an
              overview of our ICCV 2023 workshop challenge.)
              <br>
              <br>
            </div>
            This year, our challenge will consist of two tracks:

            <li>Track 1: Open-vocabulary 3D object instance search</li>
            <li>Track 2: Open-vocabulary 3D affordance grounding</li>

            <br>
          </section> -->


          <section class="section" id="Organizers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="organizers">Organizers</h2>

              <div class="columns is-centered is-variable is-0">
                <div class="column is-one-quarter">
                  <a href="http://francisengelmann.github.io">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/fang.jpg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Yi Fang</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                <div class="column is-one-quarter">
                  <a href="https://aycatakmaz.github.io">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/hao.jpeg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Hao Huang</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                

                <div class="column is-one-quarter">
                  <a href="https://github.com/WaldJohannaU">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/yu.png" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Yu Hao</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                
                <div class="column is-one-quarter">
                  <a href="https://xiwang1212.github.io/homepage/">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/geeta.png" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Geeta Chandra Raju, Bethala</p>
                            <p class="subtitle is-7">New York University, New York, USA</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>

                <!-- <div class="column is-one-fifth">
                  <a href="https://geometry.stanford.edu/member/guibas/">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/img/leonidas.jpg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Leonidas Guibas</p>
                            <p class="subtitle is-7">Stanford University</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div> -->

              </div>
            </div>
          </section>

          <!-- <section class="section" id="Sponsors">
        <div class="container is-max-desktop content">
          <h2 class="title">Sponsors </h2>
        </div>
      </section> -->

          <footer class="footer">
            <div class="container">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  <br />
                  It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
                  We would like to thank Utkarsh Sinha and Keunhong Park.
                </p>
              </div>
            </div>
          </footer>
</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
